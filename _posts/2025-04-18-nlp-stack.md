---
title: The NLP Hierarchy of Tasks
tags: ai
layout: post
---

There are many things that are considered "NLP" today. The [Call for Papers](https://2025.aclweb.org/calls/main_conference_papers/) at ACL contains over 27 tracks that do not seem to follow any structure. Or is there?

## Traditional NLP

Systems are considered to be an NLP one if they use *language knowledge* to process language data. While "counting the number of characters in a text file" is not an NLP task, counting the number of words in that same text file is an NLP task due to the requirement to know the definition of a "word" in languages. So if all NLP problems and thus systems require linguistic knowledge, we may be able to organize them using the type of knowledge that they require. To do this, we can go back to the field of linguitics (or perhaps just a school of it), which has a way to organize their knowledge based on roughly how "big" a unit the problem at hand is concern with. It goes as the following levels (quoted from SLP2, p.4):
1. **Phonetics and phonology**: knowledge about linguistic sounds (close to speech processing today?)
2. **Morphology**: knowledge about meaningful components of words (nowadays called "subwords"; such as the prefix "de-" means reversing, the "'t" means not, and s/es means plurality)
3. **Syntax**: knowledge about structural relationship between words (e.g., English follows the S-V-O structure.)
4. **Semantics**: knowledge of meaning (quite broad?)
5. **Pragmatics**: knowledge of the relationship of meaning to the goals and intentions of the speaker. (Intention is the keyword here.)
6. **Discourse**: knowledge about linguistic units larger than a single utterance (such as essays and dialogues)

This is what NLP before 2010 looks like. Almost every problem falls somewhere into this framework. My advisor works on [coreference resolution]({%post_url 2023-08-02-coreference-resolution%}) for his 2002 PhD thesis, which belongs to discourse and pragmatics (because it goes beyond sentence-level understanding, linking expressions with real-world entities). The SLP2 textbook, written in 2008, was organized pretty much around this framework. Notice the humble (but exciting) Applications section at that time.

![](/assets/slp2-contents.png)

*SLP2 table of contents*

## Contemporary NLP

But taxonomies are not fixed. Rather, they are interpretations of the epistemologists about their research area. Over time, such ares evolve. So do the taxonomies.

Let's look at the 27 areas in ACL'25 call for papers:

```markdown
    Computational Social Science and Cultural Analytics
    **Dialogue and Interactive Systems**
    **Discourse and Pragmatics**
    Efficient/Low-Resource Methods for NLP
    Ethics, Bias, and Fairness
    Generation
    Human-Centered NLP
    *Information Extraction*
    Information Retrieval and Text Mining
    Interpretability and Analysis of Models for NLP
    Language Modeling
    Linguistic theories, Cognitive Modeling and Psycholinguistics
    Machine Learning for NLP
    *Machine Translation*
    Multilinguality and Language Diversity
    Multimodality and Language Grounding to Vision, Robotics and Beyond
    NLP Applications
    **Phonology, Morphology and Word Segmentation**
    *Question Answering*
    Resources and Evaluation
    **Semantics: Lexical and Sentence-Level**
    Sentiment Analysis, Stylistic Analysis, and Argument Mining
    **Speech recognition, text-to-speech and spoken language understanding**
    *Summarization*
    **Syntax: Tagging, Chunking and Parsing**
    Special Theme: Generalization of NLP Models
```

Note that these ares are organized by alphabetical order. Why doesn't the ACL group them in a more meaningful way to make the field less confusing to the novice? I guess it is because such groupings are subjective and hard to defend. (Time to appreciate textbook writers!) I attempted to annotate on the list by **boldening** foundational tasks (i.e., appearing as a foundational task in SLP2), *italicizing* old applications, and leaving as is the other stuffs. We can see that there are quite a few "other" stuffs that don't yet fit to the textbook:

```markdown
    Computational Social Science and Cultural Analytics
    Efficient/Low-Resource Methods for NLP
    Ethics, Bias, and Fairness
    Generation
    Human-Centered NLP
    Information Retrieval and Text Mining
    Interpretability and Analysis of Models for NLP
    Language Modeling
    Linguistic theories, Cognitive Modeling and Psycholinguistics
    Machine Learning for NLP
    Multilinguality and Language Diversity
    Multimodality and Language Grounding to Vision, Robotics and Beyond
    NLP Applications
    Resources and Evaluation
    Sentiment Analysis, Stylistic Analysis, and Argument Mining
    Special Theme: Generalization of NLP Models
```

These topics are not new, because many of them already appeared in [ACL'08 call for papers](https://aclweb.org/mirror/acl2008/cfp.html). In fact, the call for papers that year did not put everything into a flat alphabetical list, but did some grouping and meaning organization. I want to analyze them, but maybe not in this post.

All of this differences and evolution in taxonomies are what makes research (specifically NLP) interesting. The reality does not care how the heck researchers organized their problem space. When real-world demand shifts and our technical understanding evolve, these taxonomies also change. The conference proceedings are the best reflection of the *active* research areas, where our understanding has not met the real-world demand. The textbooks, meanwhile, is a record of knowledge, thus also cover past advances in fields where demand has lowered.

As an anecdote on epistemology, I was once sitting in my AI class but not following the lectures. Instead, I was scribbling about the organization of research areas in AI based on the table of contents of Peter and Norvig's textbook. My friend Sharon saw that and later asked me why I did so instead of locking in the lectures. I didn't know exactly why, but I think it important to understanding such. To become a thought leader in a field, perhaps the first thing is to reverse engineer how the field's knowledge has come to place. Then the second step is to stay grounded to the reality, listen to new problems, and formulate new research directions accordingly. This is exactly the description of *position papers*, which are one of the most interesting types of papers, usually written by thought leaders in a field.

<!---
It is helpful to understand the technical foundation of NLP. 

NLP is a class of *problems*, not solutions.

The famous text-processing models such as RNN, Transformers, HMM, Naive Bayes, etc. do not inherently belong to NLP. Rather, they are *machine learning* models that are suitable to deal with the sequential data in NLP. Here, it is worthy to highlight that NLP deals a lot with *discrete sequential* data. This is in contrast to computer vision (2D or 3D spatial data), automated planning (infinitely large graphs), trading (continuous sequential data), etc.

## So, what are the problems NLP concerns with?

Overall, my prof taught that NLP is to build programs that analyze, understand, and generate human language.

But we first need to talk about the *fundamental* NLP problems. They are organized by the strutural[^1] view of human language as follows:

(add the tracks in ACL)

1. Phonetics and Phonology: how words sound
2. Morphology: how parts of a word come together and create word meanings
3. Syntax: S-V agreement, syntactic parsing, tagging, chunking,
4. Semantics: meaning at lexical and sentence-level
5. Pragmatics: context
6. Discourse: meaning beyong one single utterance (sentence)

A mind-blowing fact: NLP for these layers are primarily concerned with disambiguation. That is very insightful. Human Languages are fundamentally ambigous. When it is not ambiguous, it is 

Another nice perspective: Not all systems that process language data is an NLP system. It has to use *linguistic knowledge* to complete the task. The `wc` has an NLP part of counting words, and the non-NLP part of counting characters and lines. 


But NLP is so attractive since the beginning due to its great applications. 

- Language Modeling (for generation)
- Machine Translation
- Summarization 
- Speech processing (INTERSPEECH is more popular): speech recognition, text-to-speech (TTS) and spoken language understanding.
- Sentiment classification: Feels like it is around discourse, but it is also pretty simple. 
- Information Extraction: Used to be very important during the early days of Google Search.
- Stylistic Analysis
- Argument Mining
- Dialogue and Interactive Systems
- DEI: Ethics, Bias, and Fairness; Low-Resource NLP

## Further notes

According to [Eric Robert](https://cs.stanford.edu/people/eroberts/courses/soco/projects/2004-05/nlp/overview_history.html), NLP started in the 1940s, after WWII. Then, people wanted to do machine translation and hope to do it automatically.

SLP2 Introduction section gives an excellent overview of the field, where I learn a lot while studying NLP. However, it was from the 2007 perspective, and the author never tells us when they will finish the Third Edition of the book. 

The view of NLP from AI: Just a way for an agent to interact with the word (hearing, reading, responding). The intelligence may come from somewhere else.

[^1]: Not entirely sure if I am using the term correctly. Read more at [Structural Linguistics](https://en.wikipedia.org/wiki/Structural_linguistics).
--->

