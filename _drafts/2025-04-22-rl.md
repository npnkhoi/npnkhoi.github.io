---
title: The basics of Reinforcement Learning
tags: ml
layout: post
---

In his interview with David Silver, Lex Fridman asked him the following:

> What is reinforcement learning anyway?

David Silver is arguably one of the most famous researcher in RL. After a few gap years working in the game industry, he moved from UK to Canada for a PhD. He went to UAlberta, one of the best universities in the world for reinforcement learning research. His advisor was Charles Sutton, who was recently awarded the Turing Award 2025. And his PhD thesis was about developing a computer program that can play the game of Go better. At the time (probably about 2007), computer has beaten the human champion in the game of Chess, but not Go. Go is much more complex to Chess, with many more states, and much more complex dynamics. A few years after graduating, in 2015, he led a team called DeepMind that created AlphaGo, the first computer program that beaten the human champion in Go. He later is the main driving force at DeepMind to create AlphaZero, AlphaFold, AlphaCode, and much more.

All of that is to say, his answer to Lex's question matters. He answered as follows: Reinforcement Learning is a type of problems (not methods). Any time where you have an agent interacting with the environment, where the agent takes actions and the environment responds with rewards, it is an RL problem. Note that the agent's goal is to maximize the reward over a certain period of time, the "environment" contains states and actions connecting those states. Different from the remaining two problem classes in Machine Learning, which are supervised and unsupervised learning, RL has an inherent notion of time. In supervised learning, there are "time-series" data and problems, but they are a special type of supervised learning. 

So where do value learning, policy learning, and model learning fit in? Those are general approaches to solve an RL problem. A value function maps a state to its expected cumulative future rewards (from the agent's perspective). A policy maps a state to the optimal action to take such as the expected cumulative reward is maximized. It is trivial that optimial solutions in these two formulations are equivalent. Furthermore, one can choose to *model* the environment, i.e. deriving a function that maps a pair of state and action to the next state that the environment would bring the agent into next. In board games like Chess and Go, the model is already known and can be compactly reresented. However, in domains like automated farming, robotic manipulation, or text generation (think of RLHF), the environment is very much unknown or impossible to analytically described. However, it is not inherently required in RL to have a separate model of the environment, because the goal is just to tell the agent what to do.

Let's go on and define other popular terms in RL:
- Markov Decision Process (MDP): An environment where Markovian assumption holds, i.e., the next state only depends on the current state and the agent's action.
- Partially Observable MDP (POMDP): An MDP where the state the agent is in is not even fully observable. For example, the "state" a robot is in contains the entire observable universe there it is in, which is intractable to fully observe by the agent itself.

What is Deep RL then? (the story of AlphaGo)

RL has also been applied into NLP to guide the training of LLMs. There are a few terms to note:
- Direct Preference Optimization (DPO)
- Proximal Policy Optimization (PPO)
- GRPO


